//
//  ContentView.swift
//  jomiage
//
//  Created by aimer on 2025/03/06.
//

import AVFoundation
import SwiftUI
import Vision

import ScreenCaptureKit

class ScreenRecorder: NSObject, SCStreamOutput {
    private var stream: SCStream?
    private let captureRect: CGRect
    private let scaleFactor: CGFloat

    private var texts: [String] = .init()

    init(captureRect: CGRect, scaleFactor: CGFloat = 1.0) {
        self.captureRect = captureRect
        self.scaleFactor = scaleFactor
    }

    func startCapture() async {
//        startRecording()

        do {
            // ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã®å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤ï¼‰
            guard let display = try await SCShareableContent.current.displays.first else {
                print("Error: No display found")
                return
            }

            // ã‚­ãƒ£ãƒ—ãƒãƒ£è¨­å®š
            let config = SCStreamConfiguration()
            config.minimumFrameInterval = CMTime(value: 1, timescale: 3) // 3FPS
            config.queueDepth = 6 // ãƒãƒƒãƒ•ã‚¡ä¿æŒæ•°

            // ã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹
            stream = SCStream(filter: SCContentFilter(display: display, excludingWindows: []), configuration: config, delegate: nil)
            try stream?.addStreamOutput(self, type: .screen, sampleHandlerQueue: DispatchQueue.global())

            try await stream?.startCapture()

            print("Screen capture started")

        } catch {
            print("Error starting capture: \(error)")
        }
    }

    func stopCapture() {
        stream?.stopCapture()
        stream = nil
        print("Screen capture stopped")
    }

    // SCStreamOutput ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®å®Ÿè£…ï¼ˆç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼‰
    func stream(_ stream: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of type: SCStreamOutputType) {
        guard type == .screen, let imageBuffer = sampleBuffer.imageBuffer else {
            return
        }

        let ciImage = CIImage(cvPixelBuffer: imageBuffer)

        // captureRectã§ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ã‚¯ãƒ­ãƒƒãƒ—ã™ã‚‹
        let cropped = ciImage.cropped(to: captureRect)

        // ç”»åƒã®ç™½é»’ã‚’åè»¢
        let invertedImage = cropped.applyingFilter("CIColorInvert")

        // OCRå‡¦ç†ã‚’å®Ÿè¡Œ
        let context = CIContext()
        if let cgImage = context.createCGImage(invertedImage, from: invertedImage.extent) {
            recognizeText(from: cgImage)
        }
    }

    var beforeComment: String = ""

    func recognizeText(from image: CGImage) {
        let request = VNRecognizeTextRequest { request, _ in
            guard let observations = request.results as? [VNRecognizedTextObservation] else { return }
            for observation in observations {
                if let topCandidate = observation.topCandidates(1).first {
//                    print("Recognized Text: \(topCandidate.string)")

                    let comment = topCandidate.string

                    // ä¿¡é ¼åº¦ãŒä½ã™ãã‚‹å ´åˆç„¡è¦–ã™ã‚‹
                    if topCandidate.confidence < 0.5 {
                        continue
                    }

                    // å¤šé‡é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if self.texts.contains(comment) {
                        continue
                    }

                    // ignore list
                    if comment == "ï½›" {
                        continue
                    }

                    // çŸ­ã™ãã‚‹ã¨èª­ã¾ãªã„
                    if comment.count < 2 {
                        continue
                    }

                    // ç›¸äº’åŒ…å«
                    if true {
                        if comment.contains(self.beforeComment) {
                            continue
                        }

                        if self.beforeComment.contains(comment) {
                            continue
                        }
                    }

                    if self.hasCommonCharacters(over: 60, str1: self.beforeComment, str2: comment) {
                        print("é‡è¤‡æ’é™¤")
                        continue
                    }

                    self.beforeComment = comment

                    let hiraganaComment = self.convertToHiragana(comment)
                    self.texts.append(hiraganaComment)

                    // ç™ºå£°ã™ã‚‹
                    print("comment", comment, "hiraganaComment", hiraganaComment, "confidence", topCandidate.confidence)
                    self.synthesizeSpeech(from: topCandidate.string)
                }
            }
        }
        request.recognitionLanguages = ["ja-JP"]

        let handler = VNImageRequestHandler(cgImage: image, options: [:])
        try? handler.perform([request])
    }

    func hasCommonCharacters(over threshold: Double, str1: String, str2: String) -> Bool {
        let set1 = Set(str1)
        let set2 = Set(str2)

        let commonCount = set1.intersection(set2).count
        let totalCount = max(set1.count, set2.count) // å¤§ãã„æ–¹ã®æ–‡å­—æ•°ã‚’åŸºæº–ã«å‰²åˆè¨ˆç®—

        let commonRatio = Double(commonCount) / Double(totalCount)
        print("commonRatio", commonRatio)
        return commonRatio >= threshold / 100.0
    }

    func convertToHiragana(_ text: String) -> String {
        let locale = CFLocaleCreate(kCFAllocatorDefault, CFLocaleIdentifier("ja_JP" as CFString))
        let tokenizer = CFStringTokenizerCreate(kCFAllocatorDefault, text as CFString, CFRangeMake(0, text.utf16.count), kCFStringTokenizerUnitWord, locale)

        var result = ""

        while CFStringTokenizerAdvanceToNextToken(tokenizer) != [] {
            if let reading = CFStringTokenizerCopyCurrentTokenAttribute(tokenizer, kCFStringTokenizerAttributeLatinTranscription) {
                result += reading as! String
            } else {
                result += text
            }
        }

        return result
    }

    func isSimilarText(_ text1: String, _ text2: String) -> Bool {
        let minLength = min(text1.count, text2.count)
        let commonPrefix = text1.commonPrefix(with: text2)

        return commonPrefix.count >= minLength - 1 // 1æ–‡å­—ã®é•ã„ã¾ã§è¨±å®¹
    }

    func startRecording() {
        do {
            let audioInput = audioEngine.inputNode
            let format = audioInput.outputFormat(forBus: 0)

            audioFile = try AVAudioFile(forWriting: audioFilename, settings: format.settings)

            audioInput.installTap(onBus: 0, bufferSize: 1024, format: format) { buffer, _ in
                do {
                    try self.audioFile?.write(from: buffer)
                } catch {
                    print("Error writing audio buffer: \(error)")
                }
            }

            try audioEngine.start()
            isRecording = true
            print("Recording started path", path)
        } catch {
            print("Failed to start recording: \(error)")
        }
    }

    /// ğŸ›‘ éŒ²éŸ³ã‚’åœæ­¢ & MP3å¤‰æ›
    func stopRecording() {
//        audioEngine.stop()
//        audioEngine.inputNode.removeTap(onBus: 0)
//        isRecording = false
//        print("Recording stopped")
    }

    let synthesizer = AVSpeechSynthesizer()

    // ğŸ”¹ éŒ²éŸ³ç”¨
    private let audioEngine = AVAudioEngine()
    private var audioFile: AVAudioFile?
    private var isRecording = false
    private let path = FileManager.default.temporaryDirectory
    private let audioFilename = FileManager.default.temporaryDirectory.appendingPathComponent("recorded_audio.wav")

    func synthesizeSpeech(from text: String) {
//        print("text", text)
        let utterance = AVSpeechUtterance(string: text)
        utterance.voice = AVSpeechSynthesisVoice(language: "ja-JP")

        let voice = AVSpeechSynthesisVoice.speechVoices().filter { $0.language == "ja-JP" }
        for (i, v) in voice.enumerated() {
            if i == 7 {
                utterance.voice = v
                // éŸ³ã‚’é³´ã‚‰ã™
            }
        }

        // ã“ã“ã§ã€éŸ³ã‚’é³´ã‚‰ã—ã€éŒ²éŸ³ã™ã‚‹ã€‚
        synthesizer.speak(utterance)
    }
}

struct ContentView: View {
    var body: some View {
        VStack {
            Button("start") {
                startCapture()
            }

            Button("stop") {
                stopCapture()
            }
        }
        .padding()
    }

    @State private var screenRecorder: ScreenRecorder!

    // ç”»é¢ã®ç‰¹å®šã®ç¯„å›²ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£é–‹å§‹
    func startCapture() {
        let rect = CGRect(x: 0, y: 0, width: 1000, height: 200)

        screenRecorder = ScreenRecorder(captureRect: rect)

        Task {
            await screenRecorder.startCapture()
        }
    }

    func stopCapture() {
        screenRecorder.stopRecording()
    }
}

#Preview {
    ContentView()
}
